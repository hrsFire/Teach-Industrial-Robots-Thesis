Nach der Analyse und Auswertung der Ergebnisse können nun die in dieser Arbeit durchgeführten Arbeitsschritte resümiert und daraufhin mit einem Fazit abgerundet werden. Zu guter Letzt soll zudem auch ein Ausblick über weitere Forschungsmöglichkeiten hinsichtlich der Gestensteuerung eines Industrieroboters mittels Tiefenkameras gegeben werden.

\section{Zusammenfassung}
Es konnte aufgezeigt werden, dass eine Gestensteuerung als Alternative zu einem herkömmlichen Teach Pendant auf Grundlage von ROS mit der Zuhilfenahme eines \quoteMark{WidowX 200}-Lernroboters und einer Azure Kinect implementiert werden kann. Die entwickelten Gesten, welche einerseits unbeabsichtigte Bewegungen nicht zulassen und andererseits zudem Ermüdungserscheinungen entgegen wirken sollen, haben sich während der durchgeführten Tests bewährt. Durch die Roboter-Gesten-Anwendung wurde es einem erfahrenen Probanden hierdurch ermöglicht ein praxisnahes Teachbeispiel durchzuführen und so einem \quoteMark{WidowX 200}-Lernroboter in angemessener Zeit Zielpositionen beizubringen. Zudem ist es möglich die Zielpositionen in einer Datei zu speichern und daraufhin vom \quoteMark{WidowX 200}-Lernroboter erneut autonom abspielen zu lassen. Das erstellte Roboter-Gesten-Framework ist modular aufgebaut und kann bei Implementierung der spezifischen Schnittstellen neben dem \quoteMark{WidowX 200}-Lernroboter daher auch mit anderen Industrierobotern und neben der Azure Kinect auch mit anderen Tiefenkameras betrieben werden.\\

Die Zeit von unter 50 ms, welche für ein Teach Pendant empfohlen wird \cite[55]{prassler_advances_2004}, kann vom Azure Kinect Body Tracking SDK mit den im Durchschnitt zwischen 314,02 und 338,55 ms erzielten Latenzen nicht eingehalten werden. Aus diesem Grund muss die Berechnung des neuronalen Netzwerks der Gestenerkennung noch schneller oder auf speziell darauf ausgelegter Hardware, wie z.B. FPGAs, GPUs oder KI-Beschleuniger, durchgeführt werden \cite{welche_hardware_ki_nodate}.\\

Zudem liegt die Latenzzeit der ROS-Kommunikation in einem praxisnahen Gigabit-Ethernet-Netzwerk weit über den maximalen 50 ms, welche für ein Teach Pendant empfohlen werden \cite[55]{prassler_advances_2004}. Selbst bei Betrachtung der maximalen Durchschnittlichen Latenzzeit von 116 ms liegt diese noch zu weit über den 100 ms um einer bedienenden Person hierbei das Gefühl einer inputlag freien Erfahrung zu vermitteln \cite{miller_response_1968}. Aufgrund der extremen Schwankungen der Latenz der ROS-Kommunikation über ein Gigabit-Ethernet-Netzwerk wäre es außerdem empfehlenswert zeitkritische Steuerungsbefehle mehrmals senden zu können. Dies könnte sicherstellen, dass die Steuerungsbefehle auch zeitgerecht ankommen. Hierzu müsste jedoch ein Algorithmus implementiert werden um identische Steuerungsbefehle erkennen und ein mehrmaliges Ausführen der Steuerungsbefehle verhindern zu können. Bei der lokalen Interprozesskommunikation über das Loopback-Interface konnte sich die ROS-Kommunikation jedoch beweisen, da die Latenzzeit von Durchschnittlich zwischen 23 und 36 ms in den meisten echtzeitnahen Anwendungsgebieten ausreichend gering sein sollte. Anzumerken ist, dass die lokale Interprozesskommunikation über das Loopback-Interface dennoch immer noch ein Trade-off zwischen Modularität und Performanz darstellt. Wenn jedoch die Modularität im Vordergrund steht, kann die ROS-Kommunikation über das Loopback-Interface eine Möglichkeit darstellen um die verschiedenen Komponenten des Systems austauschbar zu halten. Das Problem besteht jedoch immer noch in der Tatsache, dass sich die Zeiten für die Gestenerkennung und der Latenzzeit zwischen der Kommunikation mit der InterbotiX-Schnittstelle summieren und dadurch weit über den erwähnten Latenzzeiten liegen. ROS ist daher keine Allzwecklösung für alle Problemstellungen, sondern nur eine mögliche Implementierung die sich erst noch über die Jahre im industriellen Einsatz behaupten muss \cite{why_dont_we_use_ros_nodate}.

\section{Ausblick}
Der CPU-Modus des Azure Kinect Body Tracking SDKs ist mit einer gemessenen Latenz von Durchschnittlich zwischen 314,02 und 338,55 ms mit minimalen Inputlags nutzbar, jedoch aber aus sicherheitstechnischer Sicht als zeitkritisch zu betrachten. Aus diesem Grund ist zu hoffen, dass das Azure Kinect Body Tracking SDK neben der CUDA basierten Grafikkartenbeschleunigung, welche nur auf Nvidia basierten Grafikkarten verfügbar ist \cite{encausse_body_nodate}, eine herstellerunabhängige Implementierung erhält. Als plattformunabhängige und alternative Programmierschnittstelle zu CUDA wäre hier z.B. OpenCL oder Vulkan anzumerken \cite{vulkan_api_2020}. Dies würde vor allem die Performanz der Gestenerkennung erhöhen, die freie Wahl der Rechnerkomponenten ermöglichen und zudem den Wettbewerb zwischen den Herstellern fördern.\\

Im Vergleich zur Kinect 2, welche eine \num{0,3}-Megapixel-Tiefenkamera verbaut hat, ist die Azure Kinect, welche eine 1-Megapixel-Tiefenkamera besitzt, bereits eine deutliche Weiterentwicklung zu ihrem Vorgängermodell. Nichtsdestotrotz ist hier jedoch durch den großen Sprung von einer \num{0,3}- auf eine 1-Megapixel-Tiefenkameraauflösung für die Zukunft noch genügend Verbesserungspotenziall vorhanden um noch höhere Auflösungen erreichen zu können \cite{bamji__2018}. Mit einer höheren Auflösung der Tiefenkamera wäre es möglich die Gesten noch genauer zu erkennen und die False-Positive-Rate noch weiter zu verringern. Mittels Eye-Tracking, welches z.B. in der HoloLens 2 verbaut ist \cite{hololens2_hardware_nodate}, könnte die Sicherheit der bedienenden Person zusätzlich erhöht und die Gestenerkennung noch zuverlässiger gestaltet werden. Hiermit könnte unter anderem erkannt werden ob die bedienende Person Ermüdungserscheinungen aufweist und daher besser eine Pause einlegen sollte oder sogar erkennen ob die bedienende Person nicht mehr bei Bewusstsein ist, weil die bedienende Person z.B. ungewollt auf dem Boden liegt. Es wäre zudem denkbar, dass die Umgebung des Industrieroboters durch weitere Tiefenkameras abgescannt werden könnte um eine Kollisionserkennung für den Industrieroboter zu realisieren. Hierdurch könnte der Industrieroboter auf Hindernisse vorzeitig reagieren und so Personen sowie auch andere Objekte frühzeitig erkennen. Dadurch könnte die Sicherheit des Systems höchstwahrscheinlich zusätzlich verbessert und die Gefahr von kritischen Situationen nochmals verringert werden. Durch die Verwendung von mehreren im Raum verteilten Tiefenkameras könnten zudem tote Winkel bei der Bedienung der Roboter-Gesten-Anwendung verringert und die Bedienung der Roboter-Gesten-Anwendung aus verschiedenen Blickrichtungen fehlerfreier ermöglicht werden.\\

Weiteres Forschungspotenzial besteht auch im Bereich der Sicherheit und Ergonomie der in dieser Arbeit entwickelten Roboter-Gesten-Anwendung. Hierbei könnte erforscht werden wie gut sich z.B. smarte Fußeinlagesohlen zur Sicherheits- und Ergonomiesteigerung eignen würden. Hierdurch würden die Hände zur Gestensteuerung frei bleiben und es könnte womöglich eine Art von \quoteMark{Dead Man's Switch} mit einer smarten Fußeinlagesohle realisiert werden. Hierdurch wäre es vermutlich machbar sehr schnelle und reflexartige Druckverteilungen und Bewegungen zu Erkennen und den Industrieroboter bei dieser Art von Bewegung zu stoppen. Zu wenig oder kein Druck auf den Fußeinlagesohlen könnten mehrere Gründe haben, da z.B. die Sohlen abgezogen wurden oder die bedienende Person ungewollt auf dem Boden liegt. Das Starten der Gestenerkennung könnte womöglich durch eine korrekt ausgeführte Kombination aus Fußbewegungen realisiert werden \cite{tan_design_2015}. Eine andere Möglichkeit würde darin bestehen einen smarten Handschuh mit einem Beschleunigungssensor und einem Gyroskop zu verwenden, welcher zu schnelle Bewegungen erkennen könnte, und die Bewegung des Industrieroboters bei zu auffälligen Bewegungen anhält \cite{ghimire_smart_2019}. Zur Realisierung der Erkennung von auffälligen Bewegungen könnte zudem ein künstliches neuronales Netzwerk auf diese auffälligen Bewegungen trainiert werden.
